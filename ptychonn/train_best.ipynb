{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a6768d9-2588-47c7-bca9-80a174a89578",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import json\n",
    "import torch, argparse, os, time, sys, shutil, logging\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "from torchsummary import summary\n",
    "from tqdm import tqdm\n",
    "from skimage.transform import resize\n",
    "from numpy.fft import fftn, fftshift\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.nn import MaxPool2d, ReLU, Linear, Upsample\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "import biotorch\n",
    "from math import log\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60b83a10-809d-4eeb-ac6d-1c25320db365",
   "metadata": {},
   "outputs": [],
   "source": [
    "numIterations = 100\n",
    "\n",
    "#adding a parseable parameter for training algorithm. \n",
    "parser = argparse.ArgumentParser(description='PtychoNN.')\n",
    "parser.add_argument('-train_alg',type=str, default=\"BP\")\n",
    "\n",
    "args, unparsed = parser.parse_known_args()\n",
    "\n",
    "# learning algorithms supported in this experiment. \n",
    "if args.train_alg==\"BP\":\n",
    "    from torch.nn import Conv2d\n",
    "elif args.train_alg==\"fa\":\n",
    "    from biotorch.layers.fa import Conv2d\n",
    "elif args.train_alg==\"dfa\":\n",
    "    from biotorch.layers.dfa import Conv2d\n",
    "elif args.train_alg==\"usf\":\n",
    "    from biotorch.layers.usf import Conv2d\n",
    "elif args.train_alg==\"frsf\":\n",
    "    from biotorch.layers.frsf import Conv2d\n",
    "elif args.train_alg==\"brsf\":\n",
    "    from biotorch.layers.brsf import Conv2d\n",
    "else :\n",
    "    print(train_alg, 'is not supported')\n",
    "\n",
    "#epochs = 60\n",
    "batch = 64\n",
    "h,w = 64,64\n",
    "nlines = 100 #How many lines of data to use for training?\n",
    "nltest = 60 #How many lines for the test set?\n",
    "n_valid = 805 #How much to reserve for validation\n",
    "\n",
    "path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06fd671e-f277-4ca3-b1c8-1189a53a862e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insertConv(layers, depth, in_channels, out_channels):\n",
    "    layers.append(Conv2d(in_channels, out_channels, 3, stride=1, padding=(1,1)))\n",
    "    layers.append(torch.nn.ReLU())\n",
    "    for i in range(depth-1):\n",
    "        layers.append(Conv2d(out_channels, out_channels, 3, stride=1, padding=(1,1)))\n",
    "        layers.append(torch.nn.ReLU())\n",
    "    \n",
    "    return layers \n",
    "\n",
    "class PtychoNN(torch.nn.Module):  \n",
    "    def __init__(self, encode_depth=2, decode_depth_1=2, decode_depth_2=2, en_filters=[], d1_filters=[], d2_filters=[]): \n",
    "        super().__init__()\n",
    "\n",
    "        #define layers in a list:\n",
    "        elayers = []\n",
    "        elayers.append(Conv2d(in_channels=1, out_channels=en_filters[0], kernel_size=3, stride=1, padding=(1,1)))\n",
    "        elayers.append(ReLU())\n",
    "        elayers = insertConv(elayers, encode_depth-1, en_filters[0], en_filters[0])\n",
    "        elayers.append(MaxPool2d((2,2)))\n",
    "        elayers = insertConv(elayers, encode_depth, en_filters[0], en_filters[1])\n",
    "        elayers.append(MaxPool2d((2,2)))\n",
    "        elayers = insertConv(elayers, encode_depth, en_filters[1], en_filters[2])\n",
    "        elayers.append(MaxPool2d((2,2)))\n",
    "\n",
    "        self.encoder = torch.nn.Sequential(*elayers)\n",
    "        \n",
    "        \n",
    "        d1layers = []\n",
    "        d1layers = insertConv(d1layers,decode_depth_1, en_filters[2], d1_filters[0])\n",
    "        d1layers.append(Upsample(scale_factor=2, mode='bilinear'))\n",
    "        d1layers = insertConv(d1layers, decode_depth_1, d1_filters[0], d1_filters[1])\n",
    "        d1layers.append(Upsample(scale_factor=2, mode='bilinear'))\n",
    "        d1layers = insertConv(d1layers, decode_depth_1, d1_filters[1], d1_filters[2])\n",
    "        d1layers.append(Upsample(scale_factor=2, mode='bilinear'))\n",
    "        d1layers.append(Conv2d(d1_filters[2], 1, 3, stride=1, padding=(1,1)))\n",
    "        d1layers.append(torch.nn.Sigmoid())\n",
    "\n",
    "        self.decoder1 = torch.nn.Sequential(*d1layers)\n",
    "       \n",
    "\n",
    "        \n",
    "        d2layers = []\n",
    "        d2layers = insertConv(d2layers, decode_depth_2, en_filters[2], d2_filters[0])\n",
    "        d2layers.append(Upsample(scale_factor=2, mode='bilinear'))\n",
    "        d2layers = insertConv(d2layers, decode_depth_2, d2_filters[0], d2_filters[1])\n",
    "        d2layers.append(Upsample(scale_factor=2, mode='bilinear'))\n",
    "        d2layers = insertConv(d2layers, decode_depth_2, d2_filters[1], d2_filters[2])\n",
    "        d2layers.append(Upsample(scale_factor=2, mode='bilinear'))\n",
    "        d2layers.append(Conv2d(d2_filters[2], 1, 3, stride=1, padding=(1,1)))\n",
    "        d2layers.append(torch.nn.Tanh())\n",
    "\n",
    "        self.decoder2 = torch.nn.Sequential(*d2layers)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x1 = self.encoder(x)\n",
    "        amp = self.decoder1(x1)\n",
    "        ph = self.decoder2(x1)\n",
    "\n",
    "        #Restore -pi to pi range\n",
    "        ph = ph*np.pi #Using tanh activation (-1 to 1) for phase so multiply by pi\n",
    "\n",
    "        return amp,ph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c61e9427-86fc-4f5c-ac53-81397d280ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data for braggNN\n",
    "def load_data():\n",
    "\n",
    "    data_diffr = np.load('data/20191008_39_diff.npz')['arr_0']\n",
    "    real_space = np.load('data/20191008_39_amp_pha_10nm_full.npy')#, allow_pickle=True)\n",
    "    amp = np.abs(real_space)\n",
    "    ph = np.angle(real_space)\n",
    "    amp.shape\n",
    "\n",
    "\n",
    "    data_diffr = np.load('data/20191008_39_diff.npz')['arr_0']\n",
    "    real_space = np.load('data/20191008_39_amp_pha_10nm_full.npy')#, allow_pickle=True)\n",
    "    amp = np.abs(real_space)\n",
    "    ph = np.angle(real_space)\n",
    "    amp.shape\n",
    "\n",
    "    #plt.matshow(np.log10(data_diffr[0,0]))\n",
    "\n",
    "    data_diffr_red = np.zeros((data_diffr.shape[0],data_diffr.shape[1],64,64), float)\n",
    "    for i in range(data_diffr.shape[0]):\n",
    "        for j in range(data_diffr.shape[1]):\n",
    "            data_diffr_red[i,j] = resize(data_diffr[i,j,32:-32,32:-32],(64,64),preserve_range=True, anti_aliasing=True)\n",
    "            data_diffr_red[i,j] = np.where(data_diffr_red[i,j]<3,0,data_diffr_red[i,j])\n",
    "\n",
    "    tst_strt = amp.shape[0]-nltest #Where to index from\n",
    "\n",
    "    X_train = data_diffr_red[:nlines,:].reshape(-1,h,w)[:,np.newaxis,:,:]\n",
    "    X_test = data_diffr_red[tst_strt:,tst_strt:].reshape(-1,h,w)[:,np.newaxis,:,:]\n",
    "    Y_I_train = amp[:nlines,:].reshape(-1,h,w)[:,np.newaxis,:,:]\n",
    "    Y_I_test = amp[tst_strt:,tst_strt:].reshape(-1,h,w)[:,np.newaxis,:,:]\n",
    "    Y_phi_train = ph[:nlines,:].reshape(-1,h,w)[:,np.newaxis,:,:]\n",
    "    Y_phi_test = ph[tst_strt:,tst_strt:].reshape(-1,h,w)[:,np.newaxis,:,:]\n",
    "\n",
    "    ntrain = X_train.shape[0]*X_train.shape[1]\n",
    "    ntest = X_test.shape[0]*X_test.shape[1]\n",
    "\n",
    "    X_train, Y_I_train, Y_phi_train = shuffle(X_train, Y_I_train, Y_phi_train, random_state=0)\n",
    "\n",
    "    #Training data\n",
    "    X_train_tensor = torch.Tensor(X_train)\n",
    "    Y_I_train_tensor = torch.Tensor(Y_I_train)\n",
    "    Y_phi_train_tensor = torch.Tensor(Y_phi_train)\n",
    "\n",
    "    #Test data\n",
    "    X_test_tensor = torch.Tensor(X_test)\n",
    "    Y_I_test_tensor = torch.Tensor(Y_I_test)\n",
    "    Y_phi_test_tensor = torch.Tensor(Y_phi_test)\n",
    "\n",
    "    train_data = TensorDataset(X_train_tensor,Y_I_train_tensor,Y_phi_train_tensor)\n",
    "    test_data = TensorDataset(X_test_tensor)\n",
    "\n",
    "    n_train = X_train_tensor.shape[0]\n",
    "    train_data2, valid_data = torch.utils.data.random_split(train_data,[n_train-n_valid,n_valid])\n",
    "\n",
    "    #download and load training data\n",
    "    trainloader = DataLoader(train_data2, batch_size=batch, shuffle=True, num_workers=4)\n",
    "    validloader = DataLoader(valid_data, batch_size=batch, shuffle=True, num_workers=4)\n",
    "    testloader = DataLoader(test_data, batch_size=batch, shuffle=False, num_workers=4)\n",
    "\n",
    "    iterations_per_epoch = np.floor((n_train-n_valid)/batch)+1 #Final batch will be less than batch size\n",
    "    step_size = 6*iterations_per_epoch\n",
    "    return trainloader, validloader, testloader, step_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd729b0d-27d3-4e22-b2a4-9928abada526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Epoch 0 - Amp Loss - 0.023781 = Phase Loss = 0.656202\n",
      "Epoch 1 - Amp Loss - 0.011935 = Phase Loss = 0.516710\n",
      "Epoch 2 - Amp Loss - 0.010288 = Phase Loss = 0.431053\n",
      "Epoch 3 - Amp Loss - 0.010288 = Phase Loss = 0.380557\n",
      "Epoch 4 - Amp Loss - 0.010288 = Phase Loss = 0.326729\n",
      "Epoch 5 - Amp Loss - 0.009660 = Phase Loss = 0.308960\n",
      "Epoch 6 - Amp Loss - 0.008470 = Phase Loss = 0.265076\n",
      "Epoch 7 - Amp Loss - 0.007945 = Phase Loss = 0.241301\n",
      "Epoch 8 - Amp Loss - 0.007653 = Phase Loss = 0.206090\n",
      "Epoch 9 - Amp Loss - 0.007484 = Phase Loss = 0.180653\n",
      "Epoch 10 - Amp Loss - 0.007322 = Phase Loss = 0.158791\n",
      "Epoch 11 - Amp Loss - 0.007191 = Phase Loss = 0.143417\n",
      "Epoch 12 - Amp Loss - 0.007186 = Phase Loss = 0.140666\n",
      "Epoch 13 - Amp Loss - 0.007186 = Phase Loss = 0.140666\n",
      "Epoch 14 - Amp Loss - 0.007186 = Phase Loss = 0.140666\n",
      "Epoch 15 - Amp Loss - 0.007186 = Phase Loss = 0.140666\n",
      "Epoch 16 - Amp Loss - 0.007186 = Phase Loss = 0.140666\n",
      "Epoch 17 - Amp Loss - 0.007186 = Phase Loss = 0.140666\n",
      "Epoch 18 - Amp Loss - 0.007186 = Phase Loss = 0.140666\n",
      "Epoch 19 - Amp Loss - 0.007092 = Phase Loss = 0.129976\n",
      "Epoch 20 - Amp Loss - 0.006972 = Phase Loss = 0.119622\n",
      "Epoch 21 - Amp Loss - 0.006914 = Phase Loss = 0.109531\n",
      "Epoch 22 - Amp Loss - 0.006827 = Phase Loss = 0.104258\n",
      "Epoch 23 - Amp Loss - 0.006792 = Phase Loss = 0.097972\n",
      "Epoch 24 - Amp Loss - 0.006792 = Phase Loss = 0.096772\n",
      "Epoch 25 - Amp Loss - 0.006780 = Phase Loss = 0.096772\n",
      "Epoch 26 - Amp Loss - 0.006780 = Phase Loss = 0.096772\n",
      "Epoch 27 - Amp Loss - 0.006780 = Phase Loss = 0.096772\n",
      "Epoch 28 - Amp Loss - 0.006780 = Phase Loss = 0.096772\n",
      "Epoch 29 - Amp Loss - 0.006780 = Phase Loss = 0.096772\n",
      "Epoch 30 - Amp Loss - 0.006780 = Phase Loss = 0.096772\n",
      "Epoch 31 - Amp Loss - 0.006780 = Phase Loss = 0.096772\n",
      "Epoch 32 - Amp Loss - 0.006684 = Phase Loss = 0.095650\n",
      "Epoch 33 - Amp Loss - 0.006643 = Phase Loss = 0.091062\n",
      "Epoch 34 - Amp Loss - 0.006610 = Phase Loss = 0.086604\n",
      "Epoch 35 - Amp Loss - 0.006580 = Phase Loss = 0.084083\n",
      "Epoch 36 - Amp Loss - 0.006549 = Phase Loss = 0.084083\n",
      "Epoch 37 - Amp Loss - 0.006549 = Phase Loss = 0.084083\n",
      "Epoch 38 - Amp Loss - 0.006549 = Phase Loss = 0.084083\n",
      "Epoch 39 - Amp Loss - 0.006549 = Phase Loss = 0.084083\n",
      "Epoch 40 - Amp Loss - 0.006549 = Phase Loss = 0.084083\n",
      "Epoch 41 - Amp Loss - 0.006549 = Phase Loss = 0.084083\n",
      "Epoch 42 - Amp Loss - 0.006461 = Phase Loss = 0.084083\n",
      "Epoch 43 - Amp Loss - 0.006461 = Phase Loss = 0.084083\n",
      "Epoch 44 - Amp Loss - 0.006374 = Phase Loss = 0.084083\n",
      "Epoch 45 - Amp Loss - 0.006350 = Phase Loss = 0.082305\n",
      "Epoch 46 - Amp Loss - 0.006313 = Phase Loss = 0.080186\n",
      "Epoch 47 - Amp Loss - 0.006236 = Phase Loss = 0.079124\n",
      "Epoch 48 - Amp Loss - 0.006228 = Phase Loss = 0.078871\n",
      "Epoch 49 - Amp Loss - 0.006202 = Phase Loss = 0.078871\n",
      "Epoch 50 - Amp Loss - 0.006202 = Phase Loss = 0.078871\n",
      "Epoch 51 - Amp Loss - 0.006202 = Phase Loss = 0.078871\n",
      "Epoch 52 - Amp Loss - 0.006202 = Phase Loss = 0.078871\n",
      "Epoch 53 - Amp Loss - 0.006202 = Phase Loss = 0.078871\n",
      "Epoch 54 - Amp Loss - 0.006155 = Phase Loss = 0.078871\n",
      "Epoch 55 - Amp Loss - 0.006119 = Phase Loss = 0.078871\n",
      "Epoch 56 - Amp Loss - 0.006090 = Phase Loss = 0.078871\n",
      "Epoch 57 - Amp Loss - 0.006090 = Phase Loss = 0.078789\n",
      "Epoch 58 - Amp Loss - 0.006056 = Phase Loss = 0.077978\n",
      "Epoch 59 - Amp Loss - 0.006048 = Phase Loss = 0.076801\n",
      "Epoch 60 - Amp Loss - 0.006027 = Phase Loss = 0.076485\n",
      "Epoch 61 - Amp Loss - 0.006027 = Phase Loss = 0.076485\n",
      "Epoch 62 - Amp Loss - 0.006027 = Phase Loss = 0.076485\n",
      "Epoch 63 - Amp Loss - 0.006027 = Phase Loss = 0.076485\n",
      "Epoch 64 - Amp Loss - 0.006027 = Phase Loss = 0.076485\n",
      "Epoch 65 - Amp Loss - 0.005984 = Phase Loss = 0.076485\n",
      "Epoch 66 - Amp Loss - 0.005984 = Phase Loss = 0.076485\n",
      "Epoch 67 - Amp Loss - 0.005984 = Phase Loss = 0.076485\n",
      "Epoch 68 - Amp Loss - 0.005959 = Phase Loss = 0.076454\n",
      "Epoch 69 - Amp Loss - 0.005936 = Phase Loss = 0.075924\n",
      "Epoch 70 - Amp Loss - 0.005909 = Phase Loss = 0.075924\n",
      "Epoch 71 - Amp Loss - 0.005909 = Phase Loss = 0.075719\n",
      "Epoch 72 - Amp Loss - 0.005909 = Phase Loss = 0.074758\n",
      "Epoch 73 - Amp Loss - 0.005906 = Phase Loss = 0.074758\n",
      "Epoch 74 - Amp Loss - 0.005878 = Phase Loss = 0.074758\n",
      "Epoch 75 - Amp Loss - 0.005878 = Phase Loss = 0.074758\n",
      "Epoch 76 - Amp Loss - 0.005871 = Phase Loss = 0.074758\n",
      "Epoch 77 - Amp Loss - 0.005871 = Phase Loss = 0.074758\n",
      "Epoch 78 - Amp Loss - 0.005871 = Phase Loss = 0.074758\n",
      "Epoch 79 - Amp Loss - 0.005858 = Phase Loss = 0.074758\n",
      "Epoch 80 - Amp Loss - 0.005858 = Phase Loss = 0.074758\n",
      "Epoch 81 - Amp Loss - 0.005837 = Phase Loss = 0.074758\n",
      "Epoch 82 - Amp Loss - 0.005835 = Phase Loss = 0.074665\n",
      "Epoch 83 - Amp Loss - 0.005835 = Phase Loss = 0.074479\n",
      "Epoch 84 - Amp Loss - 0.005802 = Phase Loss = 0.074461\n",
      "Epoch 85 - Amp Loss - 0.005802 = Phase Loss = 0.074461\n",
      "{'losses': [[0.7306163887015912, 0.06295417291464425, 0.6676622168356631], [0.5606857856281665, 0.018243243174330025, 0.5424425410372871], [0.4458080766832127, 0.01227876527563614, 0.4335293116188851], [0.3835043467643882, 0.011699466379124578, 0.3718048808704905], [0.342049867039969, 0.010212896117830978, 0.33183697016299274], [0.3136834121551834, 0.011372236503648157, 0.30231117513500344], [0.27553194579707474, 0.011918588330139633, 0.2636133565747437], [0.232227039499944, 0.007776382162055674, 0.22445065741028106], [0.20138961145607362, 0.007598949036709652, 0.19379066256414942], [0.17167906037398747, 0.007034152229668714, 0.1646449082538861], [0.1467609510076146, 0.006813688636092203, 0.13994726238130523], [0.1247434979852508, 0.00664840654290023, 0.11809509153626546], [0.11347121347774979, 0.006602428235238841, 0.10686878534425207], [0.11489561394232661, 0.006618718332963206, 0.1082768956152331], [0.12094485340248637, 0.006733026875949958, 0.11421182664001689], [0.12887411041795707, 0.007090456950079117, 0.12178365344635579], [0.13520520916130363, 0.007280852410848401, 0.12792435679741265], [0.14118904260401965, 0.007428221789994785, 0.13376082098033248], [0.13781674030949087, 0.007193380055025595, 0.13062336026620464], [0.1191288673890238, 0.006724559833068682, 0.11240430746008367], [0.10508842124533252, 0.0064843229327288246, 0.09860409805629433], [0.09366751249347414, 0.006351947426373342, 0.08731556495949], [0.0840434779201736, 0.00628725446745002, 0.07775622343315798], [0.0753724243523193, 0.00622351561719943, 0.06914890873707644], [0.07073273268692634, 0.006196782116175574, 0.06453595044357437], [0.07158242616833758, 0.006206125609905404, 0.06537630052125755], [0.07485095233100802, 0.006286068567067009, 0.0685648838480731], [0.07961853900376488, 0.006371808983776875, 0.0732467301628169], [0.08438551698286995, 0.0064451390791277425, 0.07794037789982908], [0.09021306153731186, 0.006488172881336522, 0.08372488858945229], [0.08945427957076986, 0.00638768619842076, 0.08306659344865494], [0.0823386577623231, 0.006236684089917846, 0.07610197368414462], [0.07450511165651955, 0.006140416519356375, 0.06836469507455326], [0.06892919008220945, 0.00608922859082515, 0.06283996125855365], [0.06422182072360977, 0.006038691893517345, 0.058183128765525935], [0.059276879616394766, 0.005990779101692077, 0.053286100385569725], [0.05663284416772237, 0.00596435005510492, 0.050668494181097055], [0.05711373649224514, 0.005969883322308795, 0.05114385315037074], [0.058942934858197925, 0.005996835058038475, 0.05294609977668073], [0.06152081822960818, 0.006007201455048278, 0.055513616698254054], [0.06400752721839592, 0.006017149613872797, 0.05799037748712953], [0.06713854674907292, 0.006029886132920114, 0.0611086605476732], [0.06693547336431611, 0.0059801340279239815, 0.060955339291391256], [0.06338657532669917, 0.005853761926669033, 0.0575328135878599], [0.05984930864715276, 0.005785333100283722, 0.05406397551360751], [0.056541616597971994, 0.0057289301253416955, 0.05081268642567286], [0.053694402673790435, 0.005690762679241285, 0.04800363991628675], [0.05106616204901904, 0.0056365213039949414, 0.045429640660892015], [0.04961331415639705, 0.005597707752858391, 0.04401560639571242], [0.050012291056894455, 0.0055908199356665385, 0.044421471123184474], [0.051091326572814906, 0.005615107386427767, 0.045476219233344585], [0.05232980956926065, 0.005582837137479742, 0.04674697247873835], [0.05382752009988332, 0.00559691377492825, 0.04823060638952155], [0.0554392196144126, 0.005597024962321675, 0.04984219466187373], [0.05491463774267365, 0.005564195622431756, 0.049350442083067256], [0.05313819261784313, 0.005493115839164923, 0.04764507676498229], [0.05116586574987203, 0.005442905769988644, 0.04572296003271051], [0.04980589981589999, 0.005419149670061194, 0.04438675010279447], [0.04791867621869099, 0.005382611799766035, 0.042536064246747676], [0.04652026414620776, 0.005349301942428495, 0.04117096226443263], [0.04580283279241133, 0.00533644193313828, 0.04046639092579609], [0.04611274235326202, 0.0053436900743385075, 0.04076905237088183], [0.04691517346796869, 0.00533604048810774, 0.04157913302486183], [0.047667647625844016, 0.005337095786152142, 0.04233055187686652], [0.04825109658183671, 0.005318855935269419, 0.04293224069939441], [0.048628586555729394, 0.00531222257457924, 0.04331636394397551], [0.0485567214797024, 0.0053028679734581155, 0.04325385344754748], [0.04755798850928535, 0.0052760756288913364, 0.04228191290582929], [0.04658757996972369, 0.005241134571803718, 0.04134644541357245], [0.04573224520921206, 0.005223624425854127, 0.040508620785314496], [0.044741569619093625, 0.005202259641422444, 0.03953930998549742], [0.043892317615887695, 0.0051916845557632065, 0.03870063312664753], [0.04348327706278372, 0.005170853873088705, 0.03831242323860902], [0.04382402258886009, 0.005163643047401384, 0.03866037951797998], [0.04414497603889273, 0.0051634473013965525, 0.03898152871793058], [0.04462450861680407, 0.005166376189586996, 0.03945813251330572], [0.04475897185880096, 0.005168971707340048, 0.039589999994936106], [0.04469374382571012, 0.005129597214355814, 0.039564146683747026], [0.044791165488858184, 0.005140367464921554, 0.03965079804741535], [0.04423181421491278, 0.005111591342617483, 0.03912022277838042], [0.04374572165122553, 0.005114706880765177, 0.038631014745025075], [0.04323167673179081, 0.0050965706825632, 0.03813510604140138], [0.04266749926600136, 0.0050617970757092495, 0.03760570229790291], [0.04237229696341923, 0.005070276630791922, 0.03730202035806259], [0.04197433083748617, 0.005052543932008518, 0.03692178697395725], [0.042099292084825136, 0.005044892592699963, 0.03705439945103742]], 'val_losses': [[0.6799829254547755, 0.02378060280655821, 0.6562023262182871], [0.5286449467142423, 0.011934827780351043, 0.5167101149757704], [0.4413408463199933, 0.010288168831417957, 0.43105267484982807], [0.3912760838866234, 0.010718873624379436, 0.38055720180273056], [0.3381628692150116, 0.011434008367359638, 0.3267288605372111], [0.3186206618944804, 0.00966031732968986, 0.3089603434006373], [0.27354612822333974, 0.0084702477324754, 0.2650758797923724], [0.24924621358513832, 0.007944982460079094, 0.24130123232801756], [0.21374323343237242, 0.007653264794498682, 0.2060899684826533], [0.1881371016303698, 0.0074844193101550145, 0.1806526817381382], [0.16611305003364882, 0.007322490874988337, 0.15879055857658386], [0.15060754244526228, 0.007190865270482997, 0.14341667791207632], [0.14785144726435342, 0.00718550814781338, 0.14066593907773495], [0.15280379354953766, 0.007348415092565119, 0.14545537965993086], [0.15709252282977104, 0.007404202168496947, 0.14968832458058992], [0.15941873068610826, 0.007289183946947257, 0.15212954580783844], [0.1661741981903712, 0.008832049478466312, 0.15734214708209038], [0.17842314889033636, 0.009785935593148073, 0.1686372123658657], [0.15326893515884876, 0.007223456050269306, 0.146045479302605], [0.13706720434129238, 0.007091683490822713, 0.12997551945348582], [0.12659444225331148, 0.006971990340389311, 0.11962245218455791], [0.11644444987177849, 0.006913819893573721, 0.10953063083191712], [0.11108461891611417, 0.0068267171348755555, 0.10425790088872115], [0.10476330791910489, 0.006791700647833447, 0.09797160762051742], [0.10360179655253887, 0.006829373732519646, 0.09677242425580819], [0.10575131326913834, 0.006780397302160661, 0.0989709161221981], [0.10728859342634678, 0.0069583480556805926, 0.1003302459915479], [0.1116516583909591, 0.007534388142327468, 0.10411726931730907], [0.11771659863491853, 0.007064172377189, 0.11065242687861125], [0.1209848423798879, 0.007172200712375343, 0.11381264155109723], [0.1143333874642849, 0.006790163887975116, 0.10754322508970897], [0.10666402677694957, 0.006886660470627248, 0.09977736696600914], [0.1023344062268734, 0.006684061527873079, 0.09565034446616967], [0.09770442980031173, 0.006642775804114838, 0.09106165419022243], [0.0932139412810405, 0.0066101507206136985, 0.0866037905216217], [0.09066363920768102, 0.006580337105939786, 0.0840833019465208], [0.09078737162053585, 0.006549382970357935, 0.08423799027999242], [0.09175949233273666, 0.006591769284568727, 0.08516772277653217], [0.09329724994798501, 0.006810768196980159, 0.08648648113012314], [0.09583083167672157, 0.0066074524850895005, 0.08922337989012401], [0.09629843570291996, 0.006881005208318432, 0.08941742964088917], [0.09788775071501732, 0.007056904995503525, 0.09083084451655547], [0.09558707910279433, 0.006461175352645417, 0.08912590394417445], [0.09235414179662864, 0.006483253440819681, 0.0858708880841732], [0.09122112641731898, 0.006373899173922837, 0.08484722798069318], [0.0886548204968373, 0.0063498454401269555, 0.08230497563878696], [0.08649963078399499, 0.0063132637878879905, 0.08018636641403039], [0.085359542320172, 0.006235751478622357, 0.0791237906863292], [0.08509913335243861, 0.006227985994579892, 0.07887114770710468], [0.08569479609529178, 0.00620176592686524, 0.07949303090572357], [0.08689783699810505, 0.006213497719727457, 0.08068433900674184], [0.08757711077729861, 0.006261322220476965, 0.08131578875084718], [0.08809038251638412, 0.006217099570979674, 0.08187328279018402], [0.08921868602434795, 0.006343587767332792, 0.08287509841223557], [0.0868798562635978, 0.006154785553614299, 0.08072507070998351], [0.0870541079590718, 0.006118911978167792, 0.08093519633014996], [0.08583450441559155, 0.006090251340841253, 0.07974425330758095], [0.08489108396073182, 0.006102338860121866, 0.07878874490658443], [0.0840341541916132, 0.0060563005196551485, 0.0779778528958559], [0.0828486904501915, 0.006047762503537039, 0.07680092876156171], [0.08251200802624226, 0.006027058659431835, 0.07648494963844617], [0.08378365511695544, 0.006053030180434386, 0.07773062400519848], [0.08402667567133904, 0.006042784739596148, 0.07798388972878456], [0.08416627037028472, 0.006257366544256608, 0.07790890336036682], [0.08361233087877433, 0.0060295766064276295, 0.0775827548156182], [0.08451107206443946, 0.005984334430346887, 0.07852673841019471], [0.08444108379383881, 0.0060464578758304315, 0.0783946259568135], [0.0833955140163501, 0.006033179505417745, 0.07736233373483022], [0.0824128333479166, 0.005958899273537099, 0.07645393411318462], [0.08185944147408009, 0.005935804569162428, 0.07592363655567169], [0.08195154120524724, 0.005908756012407442, 0.07604278499881427], [0.08167987937728564, 0.005960872668462495, 0.07571900635957718], [0.0807380136102438, 0.0059802825950707, 0.07475773058831692], [0.0817512155820926, 0.005905639224996169, 0.07584557682275772], [0.08179174984494846, 0.005878230168794592, 0.07591351928810279], [0.08202362805604935, 0.005914926606540878, 0.07610870152711868], [0.08172070483366649, 0.005871025573772688, 0.07584968023002148], [0.08215980976819992, 0.0058858387637883425, 0.07627397154768308], [0.08157822986443837, 0.005886959261260927, 0.0756912703315417], [0.08124884217977524, 0.005857567846154173, 0.07539127456645171], [0.08147722979386647, 0.005858457220407824, 0.07561877307792504], [0.08061123949786027, 0.005837036141504844, 0.07477420382201672], [0.08050089577833812, 0.005835462439184387, 0.07466543279588223], [0.08033986886342366, 0.005861124450651308, 0.0744787435978651], [0.0802622406433026, 0.005801601451821625, 0.07446063961833715], [0.08032649010419846, 0.005821723607368767, 0.07450476661324501]], 'lrs': [], 'best_val_loss': 0.0802622406433026, 'best_amp_loss': 0.005801601451821625, 'best_phase_loss': 0.07446063961833715}\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "loss_func = torch.nn.L1Loss()\n",
    "metrics = {'losses':[],'val_losses':[], 'lrs':[], 'best_val_loss' : np.inf, 'best_amp_loss': np.inf, 'best_phase_loss': np.inf}\n",
    "\n",
    "en_filters = [209, 120, 117]\n",
    "d1_filters = [138, 65, 117]\n",
    "d2_filters = [115, 233, 123]\n",
    "\n",
    "edepth = 2\n",
    "d1depth = 1\n",
    "d2depth = 3\n",
    "lrate = 0.000968958\n",
    "epochs = 86\n",
    "\n",
    "embed_dim = 64\n",
    "model = PtychoNN(encode_depth=edepth, decode_depth_1=d1depth, decode_depth_2=d2depth, en_filters=en_filters, d1_filters=d1_filters, d2_filters=d2_filters)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = torch.nn.DataParallel(model) #Default all devices\n",
    "\n",
    "model = model.to(device)\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lrate) \n",
    "\n",
    "# load data from the DataLoader \n",
    "# call the data loader functions directly here.\n",
    "dl_train, dl_valid, dl_test, step_size = load_data()\n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=lrate/10, max_lr=lrate, step_size_up=step_size, cycle_momentum=False, mode='triangular2')\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    #iterate through all the data in the training dataset\n",
    "    tot_loss = 0.0\n",
    "    amp_loss = 0.0\n",
    "    phs_loss = 0.0\n",
    "    \n",
    "    #train\n",
    "    for i, (tr_data, tr_amp, tr_phs) in enumerate(dl_train):\n",
    "            \n",
    "        #forward pass\n",
    "        pred_amp, pred_phs = model.forward(tr_data.to(device))\n",
    "\n",
    "        #compute the individual loss for each of the functions: \n",
    "        loss_amp = loss_func(pred_amp, tr_amp.to(device))\n",
    "        loss_phs = loss_func(pred_phs, tr_phs.to(device))\n",
    "\n",
    "        #compute the total loss: \n",
    "        loss_total = loss_amp + loss_phs\n",
    "\n",
    "        #backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss_total.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        tot_loss += loss_total.detach().item()\n",
    "        amp_loss += loss_amp.detach().item()\n",
    "        phs_loss += loss_phs.detach().item()\n",
    "\n",
    "        #Update the LR according to the schedule -- CyclicLR updates each batch\n",
    "        scheduler.step()\n",
    "\n",
    "    metrics['losses'].append([tot_loss/i,amp_loss/i,phs_loss/i])\n",
    "\n",
    "    # validate\n",
    "    tot_val_loss = 0.0\n",
    "    val_loss_amp = 0.0\n",
    "    val_loss_ph = 0.0\n",
    "        \n",
    "    for j, (ft_images,amps,phs) in enumerate(dl_valid):\n",
    "        \n",
    "        ft_images = ft_images.to(device)\n",
    "        amps = amps.to(device)\n",
    "        phs = phs.to(device)\n",
    "        pred_amps, pred_phs = model(ft_images) #Forward pass\n",
    "\n",
    "        val_loss_a = loss_func(pred_amps,amps)\n",
    "        val_loss_p = loss_func(pred_phs,phs)\n",
    "        val_loss = val_loss_a + val_loss_p\n",
    "\n",
    "        tot_val_loss += val_loss.detach().item()\n",
    "        val_loss_amp += val_loss_a.detach().item()\n",
    "        val_loss_ph += val_loss_p.detach().item()\n",
    "        \n",
    "    metrics['val_losses'].append([tot_val_loss/j,val_loss_amp/j,val_loss_ph/j])\n",
    "    \n",
    "    #Update the metrics for the individual phase and amplitude\n",
    "    if(val_loss_amp/j < metrics['best_amp_loss']):\n",
    "        metrics['best_amp_loss'] = val_loss_amp/j\n",
    "\n",
    "    #Update the metrics for the individual phase and amplitude\n",
    "    if(val_loss_ph/j < metrics['best_phase_loss']):\n",
    "        metrics['best_phase_loss'] = val_loss_ph/j\n",
    "\n",
    "    #Update saved model if val loss is lower\n",
    "    if(tot_val_loss/j<metrics['best_val_loss']):\n",
    "        #print(\"Saving improved model after Val Loss improved from %.5f to %.5f\" %(metrics['best_val_loss'],tot_val_loss/j))\n",
    "        metrics['best_val_loss'] = tot_val_loss/j\n",
    "        \n",
    "    print(\"Epoch %d - Amp Loss - %f = Phase Loss = %f\" %(epoch, metrics['best_amp_loss'], metrics['best_phase_loss']))        \n",
    "print(metrics)\n",
    "torch.save(model.state_dict(), \"final.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed6ae50-431d-4d17-ab50-459008c19ac9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
